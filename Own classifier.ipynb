{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import nltk\n",
    "from sklearn.pipeline import Pipeline\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to visualize word vector information\n",
    "def top_tfidf_feats(row, features, top_n=25):\n",
    "    ''' Get top n tfidf values in row and return them with their corresponding feature names.'''\n",
    "    topn_ids = np.argsort(row)[::-1][:top_n]\n",
    "    top_feats = [(features[i], row[i]) for i in topn_ids]\n",
    "    df = pd.DataFrame(top_feats)\n",
    "    df.columns = ['feature', 'tfidf']\n",
    "    return df\n",
    "\n",
    "def top_feats_in_doc(Xtr, features, row_id, top_n=25):\n",
    "    ''' Top tfidf features in specific document (matrix row) '''\n",
    "    row = np.squeeze(Xtr[row_id].toarray())\n",
    "    return top_tfidf_feats(row, features, top_n)\n",
    "\n",
    "def top_mean_feats(Xtr, features, grp_ids=None, min_tfidf=0.1, top_n=25, func=np.mean):\n",
    "    ''' Return the top n features that on average are most important amongst documents in rows\n",
    "        indentified by indices in grp_ids. '''\n",
    "    if grp_ids:\n",
    "        D = Xtr[grp_ids].toarray()\n",
    "    else:\n",
    "        D = Xtr.toarray()\n",
    "\n",
    "    D[D < min_tfidf] = 0\n",
    "    #tfidf_means = np.nanmean(np.where(matrix!=0,matrix,np.nan),1)\n",
    "    #tfidf_means = np.mean(D, axis=0)\n",
    "    #tfidf_means = np.sum(D, axis=0)\n",
    "    tfidf_means = func(D, axis=0)\n",
    "    return top_tfidf_feats(tfidf_means, features, top_n)\n",
    "\n",
    "def top_feats_by_class(Xtr, y, features, min_tfidf=0.1, top_n=25):\n",
    "    ''' Return a list of dfs, where each df holds top_n features and their mean tfidf value\n",
    "        calculated across documents with the same class label. '''\n",
    "    dfs = {}\n",
    "    labels = np.unique(y)\n",
    "    for label in labels:\n",
    "        ids = np.where(y==label)          \n",
    "        feats_df = top_mean_feats(Xtr, features, ids, min_tfidf=min_tfidf, top_n=top_n)\n",
    "        feats_df.label = label\n",
    "        dfs[label] = feats_df        \n",
    "    return dfs\n",
    "\n",
    "def plot_tfidf_classfeats_h(dfs):\n",
    "    ''' Plot the data frames returned by the function plot_tfidf_classfeats(). '''\n",
    "    fig = plt.figure(figsize=(12, 9), facecolor=\"w\")\n",
    "    x = np.arange(len(dfs[0]))\n",
    "    for i, df in enumerate(dfs):\n",
    "        ax = fig.add_subplot(1, len(dfs), i+1)\n",
    "        ax.spines[\"top\"].set_visible(False)\n",
    "        ax.spines[\"right\"].set_visible(False)\n",
    "        ax.set_frame_on(False)\n",
    "        ax.get_xaxis().tick_bottom()\n",
    "        ax.get_yaxis().tick_left()\n",
    "        ax.set_xlabel(\"Mean Tf-Idf Score\", labelpad=16, fontsize=14)\n",
    "        ax.set_title(\"label = \" + str(df.label), fontsize=16)\n",
    "        ax.ticklabel_format(axis='x', style='sci', scilimits=(-2,2))\n",
    "        ax.barh(x, df.tfidf, align='center', color='#3F5D7D')\n",
    "        ax.set_yticks(x)\n",
    "        ax.set_ylim([-1, x[-1]+1])\n",
    "        yticks = ax.set_yticklabels(df.feature)\n",
    "        plt.subplots_adjust(bottom=0.09, right=0.97, left=0.15, top=0.95, wspace=0.52)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "# Hamming score\n",
    "def hamming_score(y_true, y_pred, normalize=True, sample_weight=None):\n",
    "    '''\n",
    "    Compute the Hamming score (a.k.a. label-based accuracy) for the multi-label case\n",
    "    https://stackoverflow.com/q/32239577/395857\n",
    "    '''\n",
    "    acc_list = []\n",
    "    for i in range(y_true.shape[0]):\n",
    "        set_true = set( np.where(y_true[i])[0] )\n",
    "        set_pred = set( np.where(y_pred[i])[0] )\n",
    "        #print('\\nset_true: {0}'.format(set_true))\n",
    "        #print('set_pred: {0}'.format(set_pred))\n",
    "        tmp_a = None\n",
    "        if len(set_true) == 0 and len(set_pred) == 0:\n",
    "            tmp_a = 1\n",
    "        else:\n",
    "            tmp_a = len(set_true.intersection(set_pred))/\\\n",
    "                    float( len(set_true.union(set_pred)) )\n",
    "        #print('tmp_a: {0}'.format(tmp_a))\n",
    "        acc_list.append(tmp_a)\n",
    "    return np.mean(acc_list)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nro de conv. en la fecha ingresada:  553\n",
      "Nro de conv. de la carrera:  553\n",
      "Nro de conv. clasificadas:  553\n"
     ]
    }
   ],
   "source": [
    "# Get data to work with\n",
    "\n",
    "from observatorio_laboral.offer.offer_controller import OfferController\n",
    "from observatorio_laboral.offer.date_range import DateRange\n",
    "\n",
    "text_fields = []\n",
    "oc = OfferController(text_fields = [\"Job Title\", \"Description\", \"Qualifications\", \"Software\", \"Organization Name\"], table=\"train_offers\")\n",
    "date_range = DateRange(1, 2013, 5, 2017)\n",
    "source = \"symplicity\"\n",
    "\n",
    "# Get offers by date range\n",
    "oc.load_offers(source, date_range)\n",
    "print(\"Nro de conv. en la fecha ingresada: \", len(oc.offers))\n",
    "\n",
    "# Get offers by career\n",
    "oc.filter_offers_by_career(\"ECONOMÍA\")\n",
    "print(\"Nro de conv. de la carrera: \", len(oc.offers))\n",
    "\n",
    "# Get labeled offers\n",
    "oc.filter_offers_by_field(\"Areas\")\n",
    "print(\"Nro de conv. clasificadas: \", len(oc.offers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del vocabulario:  3254\n"
     ]
    }
   ],
   "source": [
    "# Optional: Ignore some classes\n",
    "offer_classes = oc.get_field_labels(\"Areas\")#, ignore=['TE', 'OI', 'EI'])\n",
    "\n",
    "# Simple text preprocesing\n",
    "offer_texts = oc.get_text()\n",
    "punctuations = ['•','/', ')', '-']\n",
    "translator = str.maketrans(\"\".join(punctuations),' '*len(punctuations))\n",
    "\n",
    "proc_data = []\n",
    "for text in offer_texts:\n",
    "    text = text.lower()\n",
    "    text = text.translate(translator)\n",
    "    proc_data.append(text)\n",
    "    \n",
    "offer_texts = proc_data\n",
    "\n",
    "X = offer_texts\n",
    "y = offer_classes\n",
    "\n",
    "# Load reviewed vocabulary\n",
    "vocab = set()\n",
    "with open(\"diccionarioEconomia.csv\") as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        word = row['Concepto']\n",
    "        mark = row['Economía']\n",
    "        \n",
    "        if mark == 's':\n",
    "            vocab.add(word)\n",
    "            \n",
    "vocab = list(vocab)\n",
    "print(\"Tamaño del vocabulario: \", len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape:  (414, 7)\n",
      "Test shape:  (139, 7)\n"
     ]
    }
   ],
   "source": [
    "# Transform labels to 0/1 arrays\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=40)\n",
    "mlb = MultiLabelBinarizer().fit(y)\n",
    "y_binary = mlb.transform(y)\n",
    "y_train_binary = mlb.transform(y_train)\n",
    "y_test_binary = mlb.transform(y_test)\n",
    "\n",
    "print(\"Train shape: \", y_train_binary.shape)\n",
    "print(\"Test shape: \", y_test_binary.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               feature     tfidf\n",
      "0             comercio  1.571429\n",
      "1    comercio exterior  1.142857\n",
      "2        importaciones  0.857143\n",
      "3          exportación  0.857143\n",
      "4             análisis  0.857143\n",
      "5           importados  0.714286\n",
      "6                 word  0.714286\n",
      "7      internacionales  0.714286\n",
      "8            logística  0.714286\n",
      "9          financieros  0.714286\n",
      "10               banca  0.571429\n",
      "11               prima  0.571429\n",
      "12               excel  0.571429\n",
      "13               point  0.571429\n",
      "14               datos  0.571429\n",
      "15       materia prima  0.571429\n",
      "16          económicas  0.571429\n",
      "17  análisis económico  0.571429\n",
      "18           económico  0.571429\n",
      "19         seguimiento  0.571429\n"
     ]
    }
   ],
   "source": [
    "vec = CountVectorizer(vocabulary=vocab, ngram_range=(1,4))\n",
    "Xtr = vec.fit_transform(X)\n",
    "features = vec.get_feature_names()\n",
    "\n",
    "for idx, label in enumerate(mlb.classes_):\n",
    "    class_y_binary = y_binary[:,idx]\n",
    "    ids = np.where(class_y_binary==1)\n",
    "    df = top_mean_feats(Xtr, features, ids, top_n=20, func=np.mean)\n",
    "    print(df)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               feature  tfidf\n",
      "0             comercio     11\n",
      "1    comercio exterior      8\n",
      "2        importaciones      6\n",
      "3          exportación      6\n",
      "4             análisis      6\n",
      "5           importados      5\n",
      "6                 word      5\n",
      "7      internacionales      5\n",
      "8            logística      5\n",
      "9          financieros      5\n",
      "10               banca      4\n",
      "11               prima      4\n",
      "12               excel      4\n",
      "13               point      4\n",
      "14               datos      4\n",
      "15       materia prima      4\n",
      "16          económicas      4\n",
      "17  análisis económico      4\n",
      "18           económico      4\n",
      "19         seguimiento      4\n"
     ]
    }
   ],
   "source": [
    "vec = CountVectorizer(vocabulary=vocab, ngram_range=(1,4))\n",
    "Xtr = vec.fit_transform(X)\n",
    "features = vec.get_feature_names()\n",
    "\n",
    "for idx, label in enumerate(mlb.classes_):\n",
    "    class_y_binary = y_binary[:,idx]\n",
    "    ids = np.where(class_y_binary==1)\n",
    "    df = top_mean_feats(Xtr, features, ids, top_n=20, func=np.sum)\n",
    "    print(df)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    feature     tfidf\n",
      "0                  comercio  0.183162\n",
      "1         comercio exterior  0.138860\n",
      "2               exportación  0.097987\n",
      "3                importados  0.087529\n",
      "4        análisis económico  0.081229\n",
      "5                económicas  0.078690\n",
      "6             importaciones  0.077289\n",
      "7           internacionales  0.075479\n",
      "8                 económico  0.068509\n",
      "9                 logística  0.060483\n",
      "10     elaboración de bases  0.059112\n",
      "11   paquetes econométricos  0.059112\n",
      "12  evaluación de políticas  0.059112\n",
      "13  evaluaciones económicas  0.059112\n",
      "14      impacto regulatorio  0.059112\n",
      "15                    banca  0.057877\n",
      "16                 política  0.056874\n",
      "17  estadísticas económicas  0.056377\n",
      "18      análisis de impacto  0.056377\n",
      "19   economía internacional  0.056377\n"
     ]
    }
   ],
   "source": [
    "vec = TfidfVectorizer(vocabulary=vocab, ngram_range=(1,4))\n",
    "Xtr = vec.fit_transform(X)\n",
    "features = vec.get_feature_names()\n",
    "\n",
    "for idx, label in enumerate(mlb.classes_):\n",
    "    class_y_binary = y_binary[:,idx]\n",
    "    ids = np.where(class_y_binary==1)\n",
    "    df = top_mean_feats(Xtr, features, ids, top_n=20, func=np.mean)\n",
    "    print(df)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EI\n",
      "EM\n",
      "FI\n",
      "MC\n",
      "OI\n",
      "PP\n",
      "TE\n"
     ]
    }
   ],
   "source": [
    "for label in mlb.classes_:\n",
    "    print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "MAX = 100\n",
    "\n",
    "def training_classifiers():\n",
    "    vec = TfidfVectorizer(vocabulary=vocab, ngram_range=(1,4))\n",
    "    Xtr = vec.fit_transform(X)\n",
    "    features = vec.get_feature_names()\n",
    "\n",
    "    class_features = {}\n",
    "    class_y_binary = {}\n",
    "    class_y_train_binary = {}\n",
    "    class_y_test_binary = {}\n",
    "    class_y_pred_binary = {}\n",
    "    class_prob_pred = {}\n",
    "    class_labels = np.zeros(shape=(len(X_test), len(mlb.classes_)))\n",
    "    class_probs = np.zeros(shape=(len(X_test), len(mlb.classes_)))\n",
    "    \n",
    "    for idx, label in enumerate(mlb.classes_):\n",
    "        class_y_binary[label] = y_binary[:,idx]\n",
    "        ids = np.where(class_y_binary[label]==1)\n",
    "        class_features[label] = top_mean_feats(Xtr, features, ids, top_n=30, func=np.sum)['feature']\n",
    "        \n",
    "        class_y_train_binary[label] = y_train_binary[:,idx]\n",
    "        \n",
    "        x1, x2, y1, y2 = train_test_split(X_train, class_y_train_binary[label], random_state=40)\n",
    "        \n",
    "        max_fs = 0\n",
    "        max_clf = None\n",
    "        max_cp = None\n",
    "        max_prior = None\n",
    "        cp_list = list(range(1,MAX))\n",
    "        cp_list.append(None)\n",
    "        none_cp = None\n",
    "        \n",
    "        for class_prior in cp_list:\n",
    "            if class_prior is None:                \n",
    "                pipeline = Pipeline([\n",
    "                    ('vec', TfidfVectorizer(vocabulary=class_features[label], ngram_range=(1,4))),\n",
    "                    ('clf', MultinomialNB())\n",
    "                ])\n",
    "            else:\n",
    "                pipeline = Pipeline([\n",
    "                    ('vec', TfidfVectorizer(vocabulary=class_features[label], ngram_range=(1,4))),\n",
    "                    ('clf', MultinomialNB(class_prior= [class_prior, MAX - class_prior]))\n",
    "                ])\n",
    "            \n",
    "            pipeline.fit(x1, y1)\n",
    "            yp = pipeline.predict(x2)\n",
    "            \n",
    "            fs = f1_score(y2, yp, average=\"binary\")\n",
    "            \n",
    "            if fs > max_fs:\n",
    "                max_fs = fs\n",
    "                max_clf = pipeline\n",
    "                max_cp = classification_report(y2, yp)\n",
    "                max_prior = class_prior\n",
    "                \n",
    "            #pr = pipeline.predict_proba(x2)            \n",
    "            #print(classification_report(y2, yp))\n",
    "            #print(f1_score(y2, yp, average=\"binary\"))\n",
    "            #break\n",
    "            \n",
    "          \n",
    "        print(\"Label: \", label)\n",
    "        print(max_prior)\n",
    "        print(max_cp)\n",
    "        print()\n",
    "        \n",
    "        class_y_test_binary[label] = y_test_binary[:, idx]\n",
    "        class_y_pred_binary[label] = max_clf.predict(X_test)\n",
    "        L = pipeline.predict_proba(X_test)[:,1]\n",
    "        class_probs[:, idx] = L\n",
    "        L[L>=0.5] = 1\n",
    "        L[L<0.5] = 0\n",
    "        class_labels[:, idx] = L\n",
    "        \n",
    "    return class_labels, class_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label:  EI\n",
      "80\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      1.00       102\n",
      "          1       1.00      0.50      0.67         2\n",
      "\n",
      "avg / total       0.99      0.99      0.99       104\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label:  EM\n",
      "36\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.40      0.56        93\n",
      "          1       0.15      0.91      0.26        11\n",
      "\n",
      "avg / total       0.89      0.45      0.53       104\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label:  FI\n",
      "33\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.81      0.51      0.63        43\n",
      "          1       0.73      0.92      0.81        61\n",
      "\n",
      "avg / total       0.76      0.75      0.74       104\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label:  MC\n",
      "70\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.90      0.94      0.92        84\n",
      "          1       0.69      0.55      0.61        20\n",
      "\n",
      "avg / total       0.86      0.87      0.86       104\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label:  OI\n",
      "74\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.98      0.98        99\n",
      "          1       0.60      0.60      0.60         5\n",
      "\n",
      "avg / total       0.96      0.96      0.96       104\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label:  PP\n",
      "37\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      0.43      0.58        49\n",
      "          1       0.65      0.95      0.77        55\n",
      "\n",
      "avg / total       0.76      0.70      0.68       104\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label:  TE\n",
      "71\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.97      0.97       100\n",
      "          1       0.40      0.50      0.44         4\n",
      "\n",
      "avg / total       0.96      0.95      0.95       104\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_binary, y_probs = training_classifiers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_pred_classifiers(X_train, y_train_binary, X_test, class_priors):\n",
    "    vec = TfidfVectorizer(vocabulary=vocab, ngram_range=(1,4))\n",
    "    Xtr = vec.fit_transform(X)\n",
    "    features = vec.get_feature_names()\n",
    "\n",
    "    class_features = {}\n",
    "    class_y_binary = {}\n",
    "    class_y_train_binary = {}\n",
    "    class_y_test_binary = {}\n",
    "    class_y_pred_binary = {}\n",
    "    class_prob_pred = {}\n",
    "    class_labels = np.zeros(shape=(len(X_test), len(mlb.classes_)))\n",
    "    class_probs = np.zeros(shape=(len(X_test), len(mlb.classes_)))\n",
    "    classifiers = {}\n",
    "    \n",
    "    for idx, label in enumerate(mlb.classes_):\n",
    "        class_y_binary[label] = y_binary[:,idx]\n",
    "        ids = np.where(class_y_binary[label]==1)\n",
    "        class_features[label] = top_mean_feats(Xtr, features, ids, top_n=30, func=np.sum)['feature']        \n",
    "        class_y_train_binary[label] = y_train_binary[:,idx]\n",
    "        \n",
    "        if class_priors[idx]:\n",
    "            pipeline = Pipeline([\n",
    "                ('vec', TfidfVectorizer(vocabulary=class_features[label], ngram_range=(1,4))),\n",
    "                ('clf', MultinomialNB(class_prior = [class_priors[idx], MAX-class_priors[idx]]))\n",
    "            ])\n",
    "        else:\n",
    "            pipeline = Pipeline([\n",
    "                ('vec', TfidfVectorizer(vocabulary=class_features[label], ngram_range=(1,4))),\n",
    "                ('clf', MultinomialNB())\n",
    "            ])\n",
    "            \n",
    "        \n",
    "        pipeline.fit(X_train, class_y_train_binary[label])\n",
    "        classifiers[label] = pipeline        \n",
    "        class_y_pred_binary[label] = pipeline.predict(X_test)\n",
    "        L = pipeline.predict_proba(X_test)[:,1]\n",
    "        class_probs[:, idx] = L\n",
    "        L[L>=0.5] = 1\n",
    "        L[L<0.5] = 0\n",
    "        class_labels[:, idx] = L\n",
    "        \n",
    "    return classifiers, class_labels, class_probs\n",
    "\n",
    "def predict(classifiers, mlb, X_test):\n",
    "    class_y_pred_binary = {}\n",
    "    class_labels = np.zeros(shape=(len(X_test), len(mlb.classes_)))\n",
    "    class_probs = np.zeros(shape=(len(X_test), len(mlb.classes_)))\n",
    "    \n",
    "    for idx, label in enumerate(mlb.classes_):        \n",
    "        class_y_pred_binary[label] = pipeline.predict(X_test)\n",
    "        L = pipeline.predict_proba(X_test)[:,1]\n",
    "        class_probs[:, idx] = L\n",
    "        L[L>=0.5] = 1\n",
    "        L[L<0.5] = 0\n",
    "        class_labels[:, idx] = L\n",
    "        \n",
    "    return class_labels, class_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_priors = [None, None, None, None, None, None, None]\n",
    "\n",
    "classifiers, y_pred_binary, probs = train_pred_classifiers(X, y_binary, X_test, class_priors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"Pickle/classifiersEconomia.p\", \"wb\") as file:\n",
    "    pickle.dump(classifiers, file)\n",
    "\n",
    "with open(\"Pickle/binarizerEconomia.p\", \"wb\") as file:\n",
    "    pickle.dump(mlb, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics :\n",
      "Accuracy: 0.504\n",
      "F1-micro: 0.723\n",
      "F1-macro: 0.320\n",
      "Hamming: 0.643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(\"Metrics :\")            \n",
    "print(\"Accuracy: %0.3f\" %  accuracy_score(y_test_binary, y_pred_binary))\n",
    "print(\"F1-micro: %0.3f\" %  f1_score(y_test_binary, y_pred_binary, average='micro'))\n",
    "print(\"F1-macro: %0.3f\" %  f1_score(y_test_binary, y_pred_binary, average='macro'))            \n",
    "print(\"Hamming: %0.3f\" %  hamming_score(y_test_binary, y_pred_binary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics :\n",
      "Accuracy: 0.439\n",
      "F1-micro: 0.674\n",
      "F1-macro: 0.296\n",
      "Hamming: 0.583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(\"Metrics :\")            \n",
    "print(\"Accuracy: %0.3f\" %  accuracy_score(y_test_binary, y_pred_binary))\n",
    "print(\"F1-micro: %0.3f\" %  f1_score(y_test_binary, y_pred_binary, average='micro'))\n",
    "print(\"F1-macro: %0.3f\" %  f1_score(y_test_binary, y_pred_binary, average='macro'))            \n",
    "print(\"Hamming: %0.3f\" %  hamming_score(y_test_binary, y_pred_binary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt = 0\n",
    "for y in y_pred_binary:\n",
    "    if 1 not in y:\n",
    "        cnt+= 1\n",
    "cnt\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1.0 0.854486686772\n",
      "0 0.0 0.406624389406\n",
      "1 1.0 0.610661428723\n",
      "1 1.0 0.679818222266\n",
      "0 1.0 0.750743136732\n",
      "0 0.0 0.258567130961\n",
      "1 0.0 0.413287069095\n",
      "1 0.0 0.495515716266\n",
      "0 0.0 0.269190339706\n",
      "0 0.0 0.245371681249\n",
      "0 0.0 0.440523884174\n",
      "0 0.0 0.279478821329\n",
      "1 1.0 0.906438153654\n",
      "0 1.0 0.581514264412\n",
      "0 0.0 0.3357188407\n",
      "1 1.0 0.851352450713\n",
      "1 1.0 0.816925137029\n",
      "1 0.0 0.403610083165\n",
      "0 0.0 0.490858346051\n",
      "0 0.0 0.315097628377\n",
      "1 1.0 0.654295791639\n",
      "1 1.0 0.761071460708\n",
      "0 1.0 0.570216827544\n",
      "1 1.0 0.718347269071\n",
      "1 1.0 0.815318643227\n",
      "1 1.0 0.63276262967\n",
      "1 1.0 0.665291912322\n",
      "0 1.0 0.558064516129\n",
      "0 1.0 0.558064516129\n",
      "0 0.0 0.43242060117\n",
      "1 0.0 0.357685893792\n",
      "1 1.0 0.847580684452\n",
      "0 0.0 0.40933578799\n",
      "1 1.0 0.685941986918\n",
      "1 0.0 0.402380284661\n",
      "0 0.0 0.235655108398\n",
      "1 0.0 0.396693556646\n",
      "1 1.0 0.912366210471\n",
      "1 1.0 0.7087047328\n",
      "0 0.0 0.245253143862\n",
      "1 0.0 0.484925112082\n",
      "1 1.0 0.624305487582\n",
      "1 1.0 0.805849788707\n",
      "0 0.0 0.245164432655\n",
      "0 0.0 0.214891565143\n",
      "0 1.0 0.505512213663\n",
      "1 1.0 0.815384154749\n",
      "0 0.0 0.32214557825\n",
      "0 0.0 0.326859005357\n",
      "1 1.0 0.841990616199\n",
      "0 0.0 0.26789062381\n",
      "1 1.0 0.736338010975\n",
      "0 0.0 0.185588864849\n",
      "0 0.0 0.393954613083\n",
      "1 1.0 0.860745653885\n",
      "1 1.0 0.718510765744\n",
      "0 0.0 0.255595447247\n",
      "1 1.0 0.905402550966\n",
      "1 1.0 0.894613287673\n",
      "0 0.0 0.283660816574\n",
      "0 0.0 0.376746855828\n",
      "1 1.0 0.719400543093\n",
      "1 1.0 0.808036670257\n",
      "0 0.0 0.233010418251\n",
      "1 1.0 0.919393766182\n",
      "0 1.0 0.751198303576\n",
      "0 0.0 0.498969724819\n",
      "1 1.0 0.60152589714\n",
      "1 1.0 0.847903095608\n",
      "1 1.0 0.740083657805\n",
      "0 1.0 0.524656621488\n",
      "0 0.0 0.305845713891\n",
      "1 1.0 0.945916431746\n",
      "1 1.0 0.637205208134\n",
      "1 1.0 0.597616156882\n",
      "1 1.0 0.804046298364\n",
      "1 1.0 0.769716767289\n",
      "0 0.0 0.214231010478\n",
      "1 1.0 0.854343578159\n",
      "1 0.0 0.454439365261\n",
      "0 0.0 0.471240164985\n",
      "1 1.0 0.731205249889\n",
      "0 0.0 0.342402393104\n",
      "1 1.0 0.785648542988\n",
      "1 1.0 0.72869631807\n",
      "0 0.0 0.246413418808\n",
      "1 1.0 0.721761289272\n",
      "0 0.0 0.273911333741\n",
      "0 0.0 0.398601483897\n",
      "0 1.0 0.585406664764\n",
      "0 0.0 0.340642966915\n",
      "0 1.0 0.568799276294\n",
      "1 0.0 0.437388406963\n",
      "0 0.0 0.305236225632\n",
      "0 0.0 0.311050633087\n",
      "1 1.0 0.780015725966\n",
      "0 1.0 0.500847275967\n",
      "1 1.0 0.709800456028\n",
      "1 1.0 0.86316784764\n",
      "1 1.0 0.797805561989\n",
      "1 1.0 0.849472006673\n",
      "1 1.0 0.837249160981\n",
      "0 0.0 0.349250235253\n",
      "0 0.0 0.25998160162\n",
      "1 1.0 0.612494358122\n",
      "1 1.0 0.573230825013\n",
      "1 1.0 0.864633141217\n",
      "1 1.0 0.840874534611\n",
      "1 1.0 0.882408020019\n",
      "0 0.0 0.294420727513\n",
      "0 1.0 0.558064516129\n",
      "0 1.0 0.602066807829\n",
      "0 1.0 0.558064516129\n",
      "1 1.0 0.616788403063\n",
      "0 0.0 0.252999141718\n",
      "0 1.0 0.590386073687\n",
      "0 1.0 0.501793351653\n",
      "0 0.0 0.230154901307\n",
      "0 1.0 0.561860457506\n",
      "1 1.0 0.854486686772\n",
      "0 0.0 0.264875381782\n",
      "1 1.0 0.856453799025\n",
      "0 0.0 0.497291420616\n",
      "0 0.0 0.485865728072\n",
      "0 0.0 0.273044522718\n",
      "1 1.0 0.913026965252\n",
      "0 1.0 0.619024731427\n",
      "0 0.0 0.273044522718\n",
      "1 1.0 0.818786273087\n",
      "0 1.0 0.734040606542\n",
      "1 1.0 0.875972900796\n",
      "0 1.0 0.626377128602\n",
      "0 0.0 0.490682297284\n",
      "1 1.0 0.618156169726\n",
      "1 0.0 0.31026823866\n",
      "0 1.0 0.628374631231\n",
      "0 0.0 0.323713720096\n",
      "1 1.0 0.880811621855\n",
      "0 1.0 0.726469311143\n"
     ]
    }
   ],
   "source": [
    "idx = 2\n",
    "for yt, yp, prob in zip(y_test_binary, y_pred_binary, y_probs):\n",
    "    print(yt[idx], yp[idx], prob[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = top_feats_by_class(Xtr, y, features, top_n=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>investigación</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>proyectos</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>desarrollo</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>investigación económica</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mining</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>participación en proyectos</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>públicos</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>tecnológicas</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>excel</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>económica</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      feature  tfidf\n",
       "0               investigación      5\n",
       "1                   proyectos      3\n",
       "2                  desarrollo      2\n",
       "3     investigación económica      1\n",
       "4                      mining      1\n",
       "5  participación en proyectos      1\n",
       "6                    públicos      1\n",
       "7                tecnológicas      1\n",
       "8                       excel      1\n",
       "9                   económica      1"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_feats_in_doc(Xtr, features, 1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offers_by_label = {}\n",
    "for doc in X_train:\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_by_label = {}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EI\n",
      "EM\n",
      "FI\n",
      "MC\n",
      "OI\n",
      "PP\n",
      "TE\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.linear_model import RidgeClassifierCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.feature_selection import SelectKBest, chi2, SelectFromModel\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "classificadores = {}\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vec',TfidfVectorizer()),\n",
    "    ('fs', SelectKBest(chi2, k=1000)),\n",
    "    ('clf', MultinomialNB()),\n",
    "])    \n",
    "\n",
    "for idx, label in enumerate(mlb.classes_):    \n",
    "    print(label)\n",
    "    classificadores[label] = Pipeline([\n",
    "        ('vec', TfidfVectorizer(max_df=0.5)),\n",
    "        ('clf', BernoulliNB()),        \n",
    "    ])\n",
    "    classificadores[label].fit(X_train, y_train_binary[:,idx])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_binary = classificadores['EI'].predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "1 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "1 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "0 0\n",
      "1 0\n",
      "0 0\n"
     ]
    }
   ],
   "source": [
    "for t, p in zip(y_test_binary[:,0], y_pred_binary):\n",
    "    print(t, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_binary=y_test_binary[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics :\n",
      "Accuracy: 0.978\n",
      "F1-micro: 0.978\n",
      "F1-macro: 0.495\n",
      "Hamming: 0.978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(\"Metrics :\")            \n",
    "print(\"Accuracy: %0.3f\" %  accuracy_score(y_test_binary, y_pred_binary))\n",
    "print(\"F1-micro: %0.3f\" %  f1_score(y_test_binary, y_pred_binary, average='micro'))\n",
    "print(\"F1-macro: %0.3f\" %  f1_score(y_test_binary, y_pred_binary, average='macro'))            \n",
    "print(\"Hamming: %0.3f\" %  hamming_score(y_test_binary, y_pred_binary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_binary[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_binary[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Maximizador()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
